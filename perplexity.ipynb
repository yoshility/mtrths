{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c5XWXNVzYs3x"
      },
      "outputs": [],
      "source": [
        "from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n",
        "from accelerate.test_utils.testing import get_backend\n",
        "\n",
        "device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n",
        "model_id = \"openai-community/gpt2-large\"\n",
        "model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n",
        "tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "0dKhVNRhZ8HS",
        "outputId": "a10bbb7d-3dcc-4c17-dfc2-e00ca6f7bfcc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.4)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ooHV1o9KY3Cr",
        "outputId": "1aff813f-5157-480d-8b15-20b6322e5fec"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Token indices sequence length is longer than the specified maximum sequence length for this model (287644 > 1024). Running this sequence through the model will result in indexing errors\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "test = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")\n",
        "encodings = tokenizer(\"\\n\\n\".join(test[\"text\"]), return_tensors=\"pt\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UE52fGXPaIb3",
        "outputId": "7594e660-437f-41f3-e698-5e3f54c910a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['text'],\n",
              "    num_rows: 4358\n",
              "})"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L2WpEgbvaKol",
        "outputId": "7c476845-c1c6-4875-d442-f386c6f0cf83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([[ 628,  796, 5199,  ...,  220,  628,  198]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1]])}"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "encodings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EmfJ8J4Y4oK",
        "outputId": "94ae214f-3a15-4cee-9d26-41513b1593cd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|█████████▉| 560/562 [04:50<00:01,  1.93it/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "max_length = model.config.n_positions # 1024\n",
        "stride = 512\n",
        "seq_len = encodings.input_ids.size(1) # 287644\n",
        "\n",
        "nll_sum = 0.0\n",
        "n_tokens = 0\n",
        "prev_end_loc = 0\n",
        "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
        "    end_loc = min(begin_loc + max_length, seq_len)\n",
        "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
        "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(input_ids, labels=target_ids)\n",
        "\n",
        "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
        "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
        "        # to the left by 1.\n",
        "        neg_log_likelihood = outputs.loss\n",
        "\n",
        "    # Accumulate the total negative log-likelihood and the total number of tokens\n",
        "    num_valid_tokens = (target_ids != -100).sum().item()  # number of valid tokens in target_ids\n",
        "    batch_size = target_ids.size(0)\n",
        "    num_loss_tokens = num_valid_tokens - batch_size  # subtract batch_size due to internal label shift\n",
        "    nll_sum += neg_log_likelihood * num_loss_tokens\n",
        "    n_tokens += num_loss_tokens\n",
        "\n",
        "    prev_end_loc = end_loc\n",
        "    if end_loc == seq_len:\n",
        "        break\n",
        "\n",
        "avg_nll = nll_sum / n_tokens  # average negative log-likelihood per token\n",
        "ppl = torch.exp(avg_nll)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h5z-ueTtcRXM",
        "outputId": "fde1bad7-d0f4-4ab3-ea12-a832179daced"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor(16.4443, device='cuda:0')"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KLLBZksnkKiC"
      },
      "outputs": [],
      "source": [
        "print(n_tokens)\n",
        "print(seq_len)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEPtGAMUDqN3"
      },
      "source": [
        "例えば、ある予測トークンの確率分布が[語彙1, 語彙2, 語彙3]=[0.7, 0.2, 0.1]だったら、\n",
        "\n",
        "そのnegative_log_likelihood(負の対数尤度)は、\n",
        "\n",
        "-log(p) (pは予測したトークンの出力確率/尤度) = -log(0.7)\n",
        "\n",
        "その予測トークンのクロスエントロピー損失は、\n",
        "\n",
        "-1×log(0.7) + -0×log(0.2) + -0×log(0.1) = -log(0.7)\n",
        "\n",
        "だから、1つのトークンに関しては、\n",
        "\n",
        "負の対数尤度 = クロスエントロピー損失 = 自信のなさ\n",
        "\n",
        "そして、perplexityは、\n",
        "\n",
        "PPL(X) = exp(負の対数尤度の平均) = exp(クロスエントロピー損失の平均)\n",
        "\n",
        "損失計算の正解ラベルに使っている文章がLLM自身の出力だから、\n",
        "\n",
        "計算した損失は、LLMが自分の答えに対してどれくらい自信がないか、つまり困惑しているかを表しているということ！"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv (3.9.5)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
